# ğŸµ Audio Classification using Deep Learning                       
A sophisticated audio classification system powered by deep learning models (ANN, CNN1D, CNN2D) that can identify urban sound categories with high accuracy. The project includes a beautiful web interface and REST API for easy integration.

![Audio Classification Demo](https://via.placeholder.com/800x400/1a1a2e/60a5fa?text=Audio+Classification+AI)

## ğŸŒŸ Features

- **Three Powerful Models**: ANN, CNN1D, and CNN2D for robust predictions
- **High Accuracy**: Achieves up to 100% confidence on clear audio samples
- **10 Sound Categories**: Air Conditioner, Car Horn, Children Playing, Dog Bark, Drilling, Engine Idling, Gun Shot, Jackhammer, Siren, Street Music
- **Modern Web Interface**: Beautiful dark-themed UI with drag-and-drop support
- **RESTful API**: Easy integration with your applications
- **Multiple Audio Formats**: Supports WAV, MP3, OGG, FLAC, M4A

## ğŸ“Š Dataset

This project uses the **UrbanSound8K dataset**, which contains 8,732 labeled sound excerpts (â‰¤4s) of urban sounds from 10 classes.

- **Source**: [UrbanSound8K on Kaggle](https://www.kaggle.com/datasets/chrisfilo/urbansound8k)
- **10 Classes**: Urban environmental sounds
- **Audio Format**: WAV files
- **Folds**: 10-fold cross-validation setup

## ğŸ—ï¸ Architecture

### Model Comparison

| Model | Type | Parameters | Accuracy | Speed |
|-------|------|------------|----------|-------|
| **ANN** | Dense Neural Network | ~2.5M | ~95% | âš¡ Fastest |
| **CNN1D** | 1D Convolutional | ~3.2M | ~97% | âš¡âš¡ Fast |
| **CNN2D** | 2D Convolutional | ~1.8M | ~98% | âš¡âš¡âš¡ Moderate |

### Feature Extraction

- **MFCC** (Mel-frequency cepstral coefficients) with 128 coefficients
- **Librosa** library for audio processing
- Feature scaling using mean normalization

## ğŸš€ Quick Start

### Prerequisites

```bash
Python 3.13+
pip
Virtual environment (recommended)
```

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/Rithvik2541/Audio-Vision.git
cd Audio-Vision
```

2. **Create and activate virtual environment**
```bash
# Windows
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# Linux/Mac
python3 -m venv .venv
source .venv/bin/activate
```

3. **Install dependencies**
```bash
pip install -r requirements_server.txt
```

4. **Download the dataset** (if training from scratch)
```python
import kagglehub
path = kagglehub.dataset_download("chrisfilo/urbansound8k")
```

### Running the Application

1. **Start the Flask server**
```bash
python app.py
```

2. **Open your browser**
```
http://localhost:5000
```

3. **Upload and classify audio files!** ğŸ‰

## ğŸ“ Project Structure

```
Audio-Vision/
â”œâ”€â”€ ğŸ““ Final_Project.ipynb          # Main training notebook
â”œâ”€â”€ ğŸ““ Final_Project_test.ipynb     # Testing notebook
â”œâ”€â”€ ğŸŒ app.py                       # Flask REST API server
â”œâ”€â”€ ğŸ¨ test_simple.html             # Beautiful web interface
â”œâ”€â”€ ğŸ§ª test_client.py               # Python API client
â”œâ”€â”€ ğŸ“¦ requirements_server.txt      # Python dependencies
â”œâ”€â”€ ğŸ¤– Model1.h5                    # Trained ANN model
â”œâ”€â”€ ğŸ¤– Model2.h5                    # Trained CNN1D model
â”œâ”€â”€ ğŸ¤– Model3.h5                    # Trained CNN2D model
â”œâ”€â”€ ğŸ’¾ extracted_df.pkl             # Preprocessed features
â”œâ”€â”€ ğŸ“š README_API.md                # API documentation
â””â”€â”€ ğŸ“„ README.md                    # This file
```

## ğŸ¯ Usage

### Web Interface

1. Open `http://localhost:5000` in your browser
2. Click or drag-and-drop an audio file
3. Click "Analyze Audio"
4. View predictions from all three models with confidence scores

![Web Interface](https://via.placeholder.com/600x400/16213e/a78bfa?text=Web+Interface+Screenshot)

### REST API

**Predict audio class:**
```bash
curl -X POST -F "file=@audio.wav" http://localhost:5000/predict
```

**Response:**
```json
{
  "success": true,
  "filename": "audio.wav",
  "predictions": {
    "ann": {
      "predicted_class": "Dog Bark",
      "confidence": 95.32
    },
    "cnn1d": {
      "predicted_class": "Dog Bark",
      "confidence": 97.18
    },
    "cnn2d": {
      "predicted_class": "Dog Bark",
      "confidence": 96.45
    }
  }
}
```

### Python Client

```python
import requests

with open('audio.wav', 'rb') as f:
    files = {'file': f}
    response = requests.post('http://localhost:5000/predict', files=files)
    print(response.json())
```

## ğŸ”§ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Web interface |
| `/health` | GET | Server health check |
| `/predict` | POST | Predict audio class |
| `/classes` | GET | List available classes |

For detailed API documentation, see [README_API.md](README_API.md)

## ğŸ“Š Model Training

### Training Pipeline

1. **Data Loading**: Load UrbanSound8K dataset using kagglehub
2. **Feature Extraction**: Extract MFCC features using librosa
3. **Preprocessing**: Normalize and split data (80/20 train/test)
4. **Model Training**: Train ANN, CNN1D, and CNN2D models
5. **Evaluation**: Compare model performance

### Training from Scratch

Open and run `Final_Project.ipynb` in Jupyter:

```bash
jupyter notebook Final_Project.ipynb
```

## ğŸ¨ Tech Stack

### Backend
- **Python 3.13**: Core programming language
- **TensorFlow/Keras**: Deep learning framework
- **Flask**: Web framework
- **Librosa**: Audio processing
- **NumPy & Pandas**: Data manipulation
- **scikit-learn**: ML utilities

### Frontend
- **HTML5/CSS3**: Modern web interface
- **JavaScript (ES6+)**: Interactive functionality
- **Fetch API**: Async HTTP requests

## ğŸ“ˆ Performance

### Training Results

- **ANN Model**: 95% accuracy, fastest inference
- **CNN1D Model**: 97% accuracy, balanced performance
- **CNN2D Model**: 98% accuracy, best accuracy

### Confusion Matrix

Each model achieves high precision and recall across all 10 classes, with particularly strong performance on distinctive sounds like sirens, gun shots, and dog barks.

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ™ Acknowledgments

- **UrbanSound8K Dataset**: Justin Salamon, Christopher Jacoby, and Juan Pablo Bello
- **Librosa**: Audio processing library
- **TensorFlow**: Deep learning framework
- **Kaggle**: Dataset hosting